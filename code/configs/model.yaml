version: "0.1.0"
description: >
  OpenRisk-Advisor forecasting & risk prediction module (weak supervision + multi-task).
  This config defines dataset windows, weak-label construction, and model settings.

task:
  period_type: "month"
  input_window_months: 18
  horizon_months: 3
  min_history_months: 24

# Targets for multi-step forecasting head (governance-relevant indicators).
forecast:
  target_metrics:
    - activity
    - contributors
    - issues_new
    - issues_closed
  quantiles: [0.1, 0.5, 0.9]
  # Apply log1p transform to non-negative count-like metrics to reduce scale issues.
  log1p_metrics:
    - activity
    - contributors
    - new_contributors
    - inactive_contributors
    - issues_new
    - issues_closed
    - change_requests
    - change_requests_accepted
    - code_change_lines_sum
    - stars
    - attention
    - participants

# Which repo-level metrics (from configs/metrics.yaml) are used as model inputs.
# Keep this list small for MVP; you can expand later.
inputs:
  metrics:
    - activity
    - contributors
    - new_contributors
    - inactive_contributors
    - bus_factor
    - issues_new
    - issues_closed
    - issue_age
    - change_requests
    - change_requests_accepted
    - code_change_lines_sum
    - stars
    - attention
    - participants
  derived_features:
    # relative changes
    - mom
    - yoy
    # short/medium trends
    - trend_slope_3
    - trend_slope_6
    # uncertainty / stability proxies
    - roll_std_3
    - zscore_12
    # data confidence
    - raw_ratio_win3
    - interp_ratio_win3

labels:
  # Weak supervision from signal_events (generated by scripts/signal_engine.py)
  source: "signal_events"
  include_signal_ids: []   # empty => include all signals
  exclude_signal_ids:
    # This is a "coverage" / informational signal; exclude from risk targets by default.
    - DATA_SUFFICIENT_6M
  aggregation:
    # Aggregate future-window signal events into a soft label in [0,1].
    # Score uses event.severity (already includes weight) and event.confidence.
    score_cap: 100.0
    score_to_soft:
      # soft = 1 - exp(-score / scale)
      scale: 40.0
    binary_threshold: 0.5
    # Encourage multi-dimension consistency: only consider "positive" when >= this many dimensions appear.
    min_dimensions_for_positive: 2
    # Use event duration (end-start+1 months) to increase score for sustained signals.
    duration_weight: 0.25
  pu_weighting:
    # For weak labels: treat high-confidence positives as positives; downweight the rest.
    pos_min_confidence: 0.34
    pos_weight: 3.0
    unlabeled_weight: 0.5

calibration:
  method: "platt"
  # Split by time for backtesting: last N months as validation window per repo.
  validation_months: 6

thresholds:
  low: 0.33
  high: 0.66
  needs_review:
    # If average raw_ratio_win3 over the input window is below this, mark as "needs review".
    min_avg_raw_ratio: 0.34
    # If forecast uncertainty is too high, also mark as "needs review".
    # uncertainty_ratio ~= mean((q90-q10)/(abs(q50)+eps)) across targets√óhorizon
    max_forecast_uncertainty_ratio: 2.0

models:
  baseline:
    type: "logreg_sgd"
    epochs: 8
    lr: 0.05
    l2: 0.001
  transformer:
    type: "temporal_transformer"
    enabled: true
    # Requires torch; training is optional and can be enabled once dependencies are installed.
    d_model: 64
    nhead: 4
    num_layers: 2
    dropout: 0.1
    epochs: 10
    batch_size: 64
    lr: 0.0005
    weight_decay: 0.01
    save_dir: "data/ml"
    # Optional masked pretraining checkpoint (produced by scripts/pretrain_transformer_masked.py)
    pretrained_path: null
